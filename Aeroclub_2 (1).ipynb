{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e329027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mrale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mrale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mrale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import math\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import CatBoostRegressor\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import locale\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')  # Download the required resource for tokenization\n",
    "nltk.download('stopwords')  # Download the required resource for stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "cat = CatBoostRegressor()\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "# загружаем список стоп-слов для русского и английского языков\n",
    "stopwords_ru = stopwords.words(\"russian\")\n",
    "stopwords_en = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a9f4fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('Aeroclub/words.json', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "places = data['dictionary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "5ebca33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/3841451493.py\", line 110, in <module>\n",
      "    df['tokens'] = df['text'].str.split()\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\", line 129, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\", line 868, in split\n",
      "    result = self._data.array._str_split(pat, n, expand, regex)\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\object_array.py\", line 353, in _str_split\n",
      "    return self._str_map(f, dtype=object)\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\object_array.py\", line 76, in _str_map\n",
      "    result = lib.map_infer_mask(arr, f, mask.view(np.uint8), map_convert)\n",
      "  File \"pandas\\_libs\\lib.pyx\", line 2786, in pandas._libs.lib.map_infer_mask\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\object_array.py\", line 331, in <lambda>\n",
      "    f = lambda x: x.split(pat, n)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\inspect.py\", line 706, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\mrale\\anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12052/3841451493.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, pat, n, expand, regex)\u001b[0m\n\u001b[0;32m    867\u001b[0m             \u001b[0mregex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_str_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\object_array.py\u001b[0m in \u001b[0;36m_str_split\u001b[1;34m(self, pat, n, expand, regex)\u001b[0m\n\u001b[0;32m    352\u001b[0m                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_str_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\object_array.py\u001b[0m in \u001b[0;36m_str_map\u001b[1;34m(self, f, na_value, dtype, convert)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_convert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer_mask\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\object_array.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    330\u001b[0m                 \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2063\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2064\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2065\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2064\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2065\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2066\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2067\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('Aeroclub/train_data.xlsx')\n",
    "df['preprocessed_texts'] = df['text']\n",
    "# Вычисляем длины строк и сортируем их\n",
    "def extract_chars_after_booking_number(df):\n",
    "    booking_number = 'по бронированию'\n",
    "    result_list = []\n",
    "\n",
    "    for text in df['preprocessed_texts']:\n",
    "        index = text.find(booking_number)\n",
    "        if index != -1:\n",
    "            start_index = index + len(booking_number)\n",
    "            chars_after_booking_number = text[start_index:start_index+40]\n",
    "            \n",
    "            if chars_after_booking_number.startswith(' '):\n",
    "                chars_after_booking_number = chars_after_booking_number.lstrip()\n",
    "            \n",
    "            result_list.append(chars_after_booking_number)\n",
    "    \n",
    "    return result_list\n",
    "\n",
    "def process111(list_book):\n",
    "    processed_list = []\n",
    "    for item in list_book:\n",
    "        while len(item) > 0 and item[0] != '1':\n",
    "            item = item[1:]\n",
    "        if len(item) > 0:\n",
    "            processed_list.append(item)\n",
    "    return processed_list\n",
    "\n",
    "def process_list_book1(list_book):\n",
    "    processed_list = []\n",
    "\n",
    "    for item in list_book:\n",
    "        last_char = item[-1]\n",
    "        while len(item) > 0 and (last_char != ')' and (not last_char.isalpha() or last_char.islower() or last_char == 'Д')):\n",
    "            item = item[:-1]\n",
    "            last_char = item[-1] if len(item) > 0 else ''\n",
    "        processed_list.append(item)\n",
    "    \n",
    "    return processed_list\n",
    "\n",
    "def remove_name_suffix(strings):\n",
    "    result = []\n",
    "    for string in strings:\n",
    "        if string.endswith('NAME'):\n",
    "            modified_string = string[:-5].rstrip()\n",
    "            result.append(modified_string)\n",
    "        else:\n",
    "            result.append(string)\n",
    "    return result\n",
    "\n",
    "def remove_after_date(strings):\n",
    "    modified_strings = []\n",
    "    for string in strings:\n",
    "        index = string.find('Дата')\n",
    "        if index != -1:\n",
    "            modified_string = string[:index].rstrip()\n",
    "            modified_strings.append(modified_string)\n",
    "        else:\n",
    "            modified_strings.append(string)\n",
    "    return modified_strings\n",
    "\n",
    "def remove_after_lower_case(strings):\n",
    "    modified_strings = []\n",
    "    for string in strings:\n",
    "        for index, char in enumerate(string):\n",
    "            if char.islower():\n",
    "                modified_string = string[:index].rstrip()\n",
    "                modified_strings.append(modified_string)\n",
    "                break\n",
    "        else:\n",
    "            modified_strings.append(string.rstrip())\n",
    "    return modified_strings\n",
    "\n",
    "def extract_chars_after_booking_number1(df):\n",
    "    booking_number = 'Номер бронирования'\n",
    "    result_list = []\n",
    "\n",
    "    for text in df['preprocessed_texts']:\n",
    "        index = text.find(booking_number)\n",
    "        if index != -1:\n",
    "            start_index = index + len(booking_number)\n",
    "            chars_after_booking_number = text[start_index:start_index+40]\n",
    "            \n",
    "            if chars_after_booking_number.startswith(' '):\n",
    "                chars_after_booking_number = chars_after_booking_number.lstrip()\n",
    "            \n",
    "            result_list.append(chars_after_booking_number)\n",
    "    \n",
    "    return result_list\n",
    "list_book = extract_chars_after_booking_number(df)\n",
    "list_book = process111(list_book)\n",
    "list_book = process_list_book1(list_book)\n",
    "list_book = remove_name_suffix(list_book)\n",
    "list_book = remove_after_date(list_book)\n",
    "list_book = remove_after_lower_case(list_book) \n",
    "list_book = remove_name_suffix(list_book)\n",
    "\n",
    "list_book1 = extract_chars_after_booking_number1(df)\n",
    "list_book1 = process111(list_book1)\n",
    "list_book1 = process_list_book1(list_book1)\n",
    "list_book1 = remove_name_suffix(list_book1)\n",
    "list_book1 = remove_after_date(list_book1)\n",
    "list_book1 = remove_after_lower_case(list_book1) \n",
    "list_book1 = remove_name_suffix(list_book1)\n",
    "\n",
    "list_book = list_book + list_book1\n",
    "\n",
    "\n",
    "df['tokens'] = df['text'].str.split()\n",
    "\n",
    "# Функция для обрезки элементов столбца до указанных ключевых слов\n",
    "def truncate_text(row):\n",
    "    keywords = ['From', 'RE', 'To', 'Sent', 'From:', 'RE:', 'To:', 'Sent:']\n",
    "    for i, token in enumerate(row):\n",
    "        if token in keywords:\n",
    "            # Если ключевое слово является первым, обрезаем до второй встречи\n",
    "            if i == 0:\n",
    "                for j, next_token in enumerate(row[i+1:]):\n",
    "                    if next_token in keywords:\n",
    "                        return row[:i+1+j]\n",
    "            # В противном случае, обрезаем до первой встречи\n",
    "            else:\n",
    "                return row[:i]\n",
    "    # Если ни одно из ключевых слов не найдено, возвращаем исходный список токенов\n",
    "    return row\n",
    "\n",
    "# Применение функции к столбцу 'tokens' и сохранение результатов в новый столбец 'trimmed_text'\n",
    "df['preprocessed_texts'] = df['tokens'].apply(truncate_text)\n",
    "\n",
    "df = df.drop('tokens', axis=1)\n",
    "df['preprocessed_texts'] = df['preprocessed_texts'].apply(' '.join)\n",
    "\n",
    "def replace_strings(list_book, df):\n",
    "    for item in list_book:\n",
    "        escaped_item = re.escape(item)\n",
    "        df['preprocessed_texts'] = df['preprocessed_texts'].str.replace(escaped_item, ' flightnum ', regex=False)\n",
    "replace_strings(list_book, df)     \n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Определение паттерна для знаков препинания\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "    \n",
    "    # Удаление знаков препинания из текста\n",
    "    text_without_punctuation = re.sub(punctuation_pattern, '', text)\n",
    "    \n",
    "    return text_without_punctuation\n",
    "\n",
    "# Применение функции remove_punctuation к столбцу 'text' в DataFrame\n",
    "df['preprocessed_texts'] = df['preprocessed_texts'].apply(remove_punctuation)\n",
    "df['preprocessed_texts'] = df['preprocessed_texts'].apply(lambda x: 'ничего' if x.isspace() else x)\n",
    "df['name_count'] = df['preprocessed_texts'].apply(lambda x: x.count('NAME'))\n",
    "df['preprocessed_texts'] = df['preprocessed_texts'].apply(lambda x: re.sub(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1', x))\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    russian_stopwords = stopwords.words('russian')\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    lemmatizer_en = WordNetLemmatizer()\n",
    "    lemmatizer_ru = MorphAnalyzer()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.lower() in russian_stopwords or word.lower() in english_stopwords:\n",
    "            continue\n",
    "        if word.isalpha() and any(c.isalpha() for c in word):\n",
    "            cleaned_words.append(lemmatizer_ru.parse(word)[0].normal_form)\n",
    "        elif word.isalpha():\n",
    "            cleaned_words.append(lemmatizer_en.lemmatize(word))\n",
    "    \n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    return cleaned_text\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "df['preprocessed_texts'] = df['preprocessed_texts'].progress_apply(preprocess_text)\n",
    "\n",
    "df['preprocessed_texts'] = df['preprocessed_texts'].fillna('ничего')\n",
    "df['preprocessed_texts'] = df['preprocessed_texts'].replace(r'^\\s*$', 'ничего', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "ca040636",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data_aeroclub.xlsx')\n",
    "df.rename(columns={'preprocessed_texts': 'preprocessed_text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "38a52c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word not in stopwords_ru and word not in stopwords_en]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words]\n",
    "    russian_stopwords = stopwords.words('russian')\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    lemmatizer_en = WordNetLemmatizer()\n",
    "    lemmatizer_ru = MorphAnalyzer()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.lower() in russian_stopwords or word.lower() in english_stopwords:\n",
    "            continue\n",
    "        if word.isalpha() and any(c.isalpha() for c in word):\n",
    "            cleaned_words.append(lemmatizer_ru.parse(word)[0].normal_form)\n",
    "        elif word.isalpha():\n",
    "            cleaned_words.append(lemmatizer_en.lemmatize(word))\n",
    "    \n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "word_list = [\n",
    "    \"Заявка\",\n",
    "    \"Запрос\",\n",
    "    \"Прошу\",\n",
    "    \"Просьба\",\n",
    "    \"Бронировать\",\n",
    "    \"Оформление\",\n",
    "    \"Гостиница\",\n",
    "    \"Командировка\",\n",
    "    \"Регистрация\",\n",
    "    \"Номер\",\n",
    "    \"Проживание\",\n",
    "    \"Рейс\", 'name', 'email', 'phone', 'коллеги', 'номер бронирования', 'пассажиры', 'вариант','flightnum']\n",
    "\n",
    "stop_list = ['Услуга ожидает обработки', 'оставил следующее сообщение','изменен статус','прикрепленном файле','Архивную услугу',\"Service Desk\"]\n",
    "#locations = pd.read_excel('Locations.xlsx')\n",
    "preprocessed_words = [preprocess_text(word) for word in word_list]\n",
    "stop_list = [preprocess_text(word) for word in stop_list]\n",
    "#preprocessed_df = locations[['NameRu', 'NameEn']].applymap(preprocess_text)\n",
    "#aero_locations = preprocessed_df.values.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5bf98556",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'].fillna('Ничего', inplace=True)\n",
    "places = {key: value for key, value in places.items() if key.count('name') <= 1}\n",
    "new_words_value = 2 * sum(list(places.values())) / len(list(places.values()))\n",
    "places.update({word: new_words_value for word in preprocessed_words})\n",
    "\n",
    "sorted_dict = sorted(places.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Находим пороговое значение для топ-10%\n",
    "threshold = sorted_dict[math.floor(len(sorted_dict) * 0.1)][1]\n",
    "\n",
    "# Заменяем топ 10% самых больших значений на threshold\n",
    "for key, value in places.items():\n",
    "    if value >= threshold:\n",
    "        places[key] = threshold\n",
    "\n",
    "\n",
    "df['preprocessed_text'].fillna('Ничего', inplace=True)\n",
    "def count_words(preprocessed_text):\n",
    "    words = nltk.word_tokenize(preprocessed_text)\n",
    "    return len(words)\n",
    "df['length'] = df['preprocessed_text'].apply(lambda x: count_words(x))\n",
    "\n",
    "df['target'] = df['preprocessed_text'].apply(lambda x: 0 if any(stop_word in x for stop_word in stop_list) else math.log(1 + sum([places[word] for word in x.split() if word in places])))\n",
    "df['target'] = df['target'] / df['length'].apply(lambda x: x)\n",
    "\n",
    "\n",
    "\n",
    "word_counts = {word: [text.count(word) for text in df['preprocessed_text']] for word in places.keys()}\n",
    "\n",
    "y = df['target']\n",
    "x = pd.DataFrame(word_counts)\n",
    "x['length'] = df['length']\n",
    "from catboost import Pool, cv\n",
    "cat.fit(x,y,verbose=False, plot=False)\n",
    "df['pred1'] = cat.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "fa1d2d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "df['result'] = df['target']*df['pred1']\n",
    "top_10_cutoff = df['target'].quantile(0.9)\n",
    "bottom_10_cutoff = df['target'].quantile(0.1)\n",
    "\n",
    "# Filter df to get df1 with top 10% values\n",
    "df1 = df[df['target'] >= top_10_cutoff]\n",
    "\n",
    "# Filter df to get df2 with smallest 10% values\n",
    "df2 = df[df['target'] <= bottom_10_cutoff]\n",
    "word_freq1 = {}\n",
    "word_freq2 = {}\n",
    "# Iterate over each text in the 'text' column\n",
    "for text in df1['preprocessed_text']:\n",
    "    # Split the text into individual words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Update the word frequencies in the dictionary\n",
    "    for word in words:\n",
    "        word_freq1[word] = word_freq1.get(word, 0) + 1\n",
    "\n",
    "for text in df2['preprocessed_text']:\n",
    "    # Split the text into individual words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Update the word frequencies in the dictionary\n",
    "    for word in words:\n",
    "        word_freq2[word] = word_freq2.get(word, 0) + 1\n",
    "\n",
    "\n",
    "word_freq1 = {word: freq for word, freq in word_freq1.items() if freq >= 10}\n",
    "word_freq2 = {word: freq for word, freq in word_freq2.items() if freq >= 10}\n",
    "\n",
    "common_keys = set(word_freq1.keys()) & set(word_freq2.keys())\n",
    "word_freq1 = {word: freq for word, freq in word_freq1.items() if word not in common_keys}\n",
    "word_freq2 = {word: freq for word, freq in word_freq2.items() if word not in common_keys}\n",
    "\n",
    "max_value1 = max(word_freq1.values())\n",
    "min_value1 = min(word_freq1.values())\n",
    "\n",
    "word_freq1 = {word: (freq - min_value1) / (max_value1 - min_value1) for word, freq in word_freq1.items()}\n",
    "max_value2 = max(word_freq2.values())\n",
    "min_value2 = min(word_freq2.values())\n",
    "\n",
    "word_freq2 = {word: (freq - min_value2) / (max_value2 - min_value2) for word, freq in word_freq2.items()}\n",
    "min_length = min(len(word_freq1), len(word_freq2))\n",
    "\n",
    "# Сортировка словарей по убыванию значений value\n",
    "sorted_word_freq1 = sorted(word_freq1.items(), key=lambda x: x[1], reverse=True)[:min_length]\n",
    "sorted_word_freq2 = sorted(word_freq2.items(), key=lambda x: x[1], reverse=True)[:min_length]\n",
    "\n",
    "# Создание новых словарей с одинаковой длиной\n",
    "word_freq1 = dict(sorted_word_freq1)\n",
    "word_freq2 = dict(sorted_word_freq2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c37dfa67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_freq1 = dict(word_freq1)\n",
    "word_freq2 = dict(word_freq2)\n",
    "word_freq1.update({word: new_words_value for word in preprocessed_words})\n",
    "word_freq2.update({word: new_words_value for word in preprocessed_words})\n",
    "word_freq1 = sorted(word_freq1.items(), key=lambda x: x[1], reverse=True)\n",
    "word_freq2 = sorted(word_freq2.items(), key=lambda x: x[1], reverse=True)\n",
    "# Находим пороговое значение для топ-10%\n",
    "threshold1 = word_freq1[math.floor(len(word_freq1) * 0.1)][1]\n",
    "threshold2 = word_freq2[math.floor(len(word_freq2) * 0.1)][1]\n",
    "# Заменяем топ 10% самых больших значений на threshold\n",
    "word_freq1 = dict(word_freq1)\n",
    "word_freq2 = dict(word_freq2)\n",
    "for key, value in word_freq1.items():\n",
    "    if value >= threshold1:\n",
    "        word_freq1[key] = threshold\n",
    "\n",
    "for key, value in word_freq2.items():\n",
    "    if value <= threshold2:\n",
    "        word_freq2[key] = threshold\n",
    "\n",
    "        \n",
    "df['target_1'] = df['preprocessed_text'].apply(lambda x: math.log(1 + sum([word_freq1[word] for word in x.split() if word in word_freq1])))\n",
    "\n",
    "df['target_2'] = df['preprocessed_text'].apply(lambda x: math.log(1 + sum([word_freq2[word] for word in x.split() if word in word_freq2])))\n",
    "\n",
    "df['target2'] = df['target_1'] - df['target_2']\n",
    "df['target2'] = df['target2'] / df['length'].apply(lambda x: x)\n",
    "\n",
    "word_counts1 = {word: [text.count(word) for text in df['preprocessed_text']] for word in word_freq1.keys()}\n",
    "word_counts2 = {word: [text.count(word) for text in df['preprocessed_text']] for word in word_freq2.keys()}\n",
    "\n",
    "word_counts1.update(word_counts2)\n",
    "\n",
    "y = df['target2']\n",
    "x = pd.DataFrame(word_counts1)\n",
    "x['length'] = df['length']\n",
    "#grid = {'learning_rate': [0.1]}\n",
    "cat2 = CatBoostRegressor()\n",
    "#grid_search_result = cat.grid_search(grid, x, y=y, plot=False)\n",
    "cat2.fit(x,y,verbose=False, plot=False)\n",
    "\n",
    "\n",
    "df['pred2'] = cat2.predict(x)\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "264252fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mrale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mrale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1970068260.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('data_aeroclub.xlsx')\n",
    "df.rename(columns={'preprocessed_texts': 'preprocessed_text'}, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preprocess the text\n",
    "nltk.download('punkt')  # Download the required resource for tokenization\n",
    "nltk.download('stopwords')  # Download the required resource for stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('russian'))  # Set the stopwords language\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and convert to lowercase\n",
    "        tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "        return tokens\n",
    "    else:\n",
    "        return [] \n",
    "\n",
    "# Apply the preprocessing to the text column\n",
    "#df_test['processed_text'] = df_test['preprocessed_text'].apply(preprocess_text)\n",
    "\n",
    "# Train the Word2Vec model\n",
    "sentences = df_test['processed_text'].tolist()\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "keywords = preprocessed_words[:]\n",
    "len(keywords)\n",
    "similarity = {}\n",
    "for k in keywords:\n",
    "    similarity[k] = 1\n",
    "similarity = dict(sorted(similarity.items(), key=lambda x: x[-1], reverse=True))\n",
    "for i in range(100):\n",
    "    similar_words = {}\n",
    "    arr_w = []\n",
    "    for keyword in keywords:\n",
    "        if keyword in model.wv.vocab:\n",
    "            similar_words[keyword] = model.wv.most_similar(keyword, topn=1)\n",
    "            for k, v in similar_words[keyword]:\n",
    "                similarity[k] = max(v, similarity.get(k, 0))\n",
    "                if k not in keywords:\n",
    "                    arr_w.append(k)\n",
    "        else:\n",
    "            similar_words[keyword] = []\n",
    "    keywords.extend(arr_w)\n",
    "    if len(keywords) >= 70:\n",
    "        break\n",
    "\n",
    "similarity = {}\n",
    "for k in keywords:\n",
    "    similarity[k] = 1\n",
    "\n",
    "most_similar_words = {}\n",
    "for keyword in keywords:\n",
    "    if keyword in model.wv.vocab:\n",
    "        most_similar_words[keyword] = model.wv.most_similar(keyword, topn=7)\n",
    "        for k, v in most_similar_words[keyword]:\n",
    "            similarity[k] = max(v, similarity.get(k, 0))\n",
    "    else:\n",
    "        most_similar_words[keyword] = []\n",
    "\n",
    "less_similar_words = {}\n",
    "for keyword in keywords:\n",
    "    if keyword in model.wv.vocab:\n",
    "        less_similar_words[keyword] = model.wv.most_similar(negative=[keyword], topn=3)\n",
    "        for k, v in less_similar_words[keyword]:\n",
    "            similarity[k] = min(-1*v, similarity.get(k, 0))\n",
    "    else:\n",
    "        less_similar_words[keyword] = []\n",
    "\n",
    "similar_best = list(similarity.keys())[:150]\n",
    "similar_worst = list(similarity.keys())[-50:]\n",
    "\n",
    "for word in similar_best:\n",
    "    df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)\n",
    "for word in similar_worst:\n",
    "    df[word] = df.apply(lambda row: str(row['preprocessed_text']).count(word) * similarity[word], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "44282d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "\n",
    "def count_words(preprocessed_text):\n",
    "    words = nltk.word_tokenize(preprocessed_text)\n",
    "    return len(words)\n",
    "df.dropna(inplace=True)\n",
    "df['length'] = df['text'].apply(lambda x: count_words(x))\n",
    "df['target'] = df[numeric_columns].sum(axis=1)\n",
    "df['target'] = df['target'] / df['length'].apply(lambda x: x)\n",
    "df.dropna(inplace=True)\n",
    "X, Y = df.iloc[:, 4:-1], df['target']\n",
    "dog = CatBoostRegressor()\n",
    "#grid_search_result = cat.grid_search(grid, x, y=y, plot=False)\n",
    "dog.fit(X,Y,verbose=False, plot=False)\n",
    "df['pred2'] = dog.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "9e678542",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df.merge(data, on='text', how='inner')\n",
    "\n",
    "# Keep only the rows from df and data that have matching values in the 'text' column\n",
    "df_filtered = df[df['text'].isin(merged_df['text'])]\n",
    "data_filtered = data[data['text'].isin(merged_df['text'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "70f7b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered.iloc[:, 3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "91e4bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[x.index.isin(X.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "b86d8456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1333176468.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered[f'pred{i+3}'] = globals()[f'superdog{i}'].predict(x)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1333176468.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered[f'pred{i+3}'] = globals()[f'superdog{i}'].predict(x)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1333176468.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered[f'pred{i+3}'] = globals()[f'superdog{i}'].predict(x)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1333176468.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered[f'pred{i+3}'] = globals()[f'superdog{i}'].predict(x)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp/ipykernel_12052/1333176468.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered[f'pred{i+3}'] = globals()[f'superdog{i}'].predict(x)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(5):\n",
    "    globals()[f'supercat{i}'] = CatBoostRegressor()\n",
    "    globals()[f'superdog{i}'] = CatBoostRegressor()\n",
    "    \n",
    "    # Создание буферных копий df_filtered и data_filtered\n",
    "    df_filtered_buffer = df_filtered.copy()\n",
    "    data_filtered_buffer = data_filtered.copy()\n",
    "    \n",
    "    y_col = f'pred{i+2}'\n",
    "    Y_col = f'pred{i+2}'\n",
    "    y = data_filtered_buffer[y_col]\n",
    "    Y = df_filtered_buffer[Y_col]\n",
    "    \n",
    "    columns_to_remove = [col for col in df_filtered_buffer.columns if re.search(r'(?:target|pred)(?:_\\d+)?', str(col))]\n",
    "    columns_to_remove = [col for col in columns_to_remove if col in df_filtered_buffer.columns]\n",
    "    df_filtered_buffer = df_filtered_buffer.drop(columns=columns_to_remove, errors='ignore')\n",
    "    \n",
    "    columns_to_remove = [col for col in data_filtered_buffer.columns if re.search(r'(?:target|pred)(?:_\\d+)?', str(col))]\n",
    "    columns_to_remove = [col for col in columns_to_remove if col in data_filtered_buffer.columns]\n",
    "    data_filtered_buffer = data_filtered_buffer.drop(columns=columns_to_remove, errors='ignore')\n",
    "    \n",
    "    globals()[f'superdog{i}'].fit(x, y, verbose=False, plot=False)\n",
    "    globals()[f'supercat{i}'].fit(X, Y, verbose=False, plot=False)\n",
    "    \n",
    "    df_filtered[f'pred{i+3}'] = globals()[f'supercat{i}'].predict(X)\n",
    "    data_filtered[f'pred{i+3}'] = globals()[f'superdog{i}'].predict(x)\n",
    "    X,x = x,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "776f01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxim = 0\n",
    "pred= 0\n",
    "words = 0\n",
    "for j in range(18,19):\n",
    "    for i in range(2,5):\n",
    "        list1 = data_filtered.nlargest(i*500, f'pred{j+3}').index\n",
    "        list2 = df_filtered.nlargest(i*500, f'pred{j+3}').index\n",
    "\n",
    "        common_elements = set(list1).intersection(list2)\n",
    "        count_common_elements = len(common_elements)\n",
    "        if count_common_elements/(i*500) > maxim:\n",
    "            maxim = count_common_elements/(i*500)\n",
    "            pred = j\n",
    "            words = 500*i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "037f9b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5206666666666667"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ad065094",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_rows = df_filtered[df_filtered['pred12'].isin(data_filtered['pred12'].nlargest(1000))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "fbdf3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_rows = df_filtered.nlargest(50, 'pred7')\n",
    "text_column = top_10_rows['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "72bced09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12535    Добрый день, NAME, Пож - та уточните стоимость...\n",
       "12436    NAME, добрый день, NAME Пож - та уточните стои...\n",
       "12653    NAME, добрый день, Просьба уточнить стоимость ...\n",
       "12359    Добрый день, NAME, Просьба уточнить стоимость ...\n",
       "12559    NAME, добрый день, Уточните пож - та стоимость...\n",
       "12817    NAME, добрый день, Пож - та уточните пож - та ...\n",
       "12457    NAME, добрый день, Пож - та уточните стоимость...\n",
       "12560    NAME, добрый день, Уточните пож - та стоимость...\n",
       "12734    NAME, добрый день, NAME Просьба уточнить стоим...\n",
       "12881    NAME, добрый день, Пришлите пож - та стоимость...\n",
       "12434    NAME, добрый день, Уточните пож - та стоимость...\n",
       "13133    NAME, доброе утро, NAME Пож - та пришлите стои...\n",
       "12363    NAME, добрый день, NAME Пришлите пож - та стои...\n",
       "12896    NAME, добрый день, Просьба забронировать для N...\n",
       "12733    NAME, добрый день, NAME Пож - та забронируйте ...\n",
       "12358     Рита, добрый день, NAME Пришлите пож - та сто...\n",
       "3440     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "3396     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "2081     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "2393     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "13553     Рита, NAME По билету для NAME NAME, Уфа. NAME...\n",
       "13090    Добрый день! Забронируйте, пожалуйста, билеты ...\n",
       "2847     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "1093     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "1785     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "2082     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "32       Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "9403     Добрый день! NAME Планирую командировку в г. N...\n",
       "11064    Добрый день, Пожалуйста, забронируйте рейсы: 1...\n",
       "12812    Добрый день, Пожалуйста, забронируйте рейсы: С...\n",
       "12963     Рита, добрый день, Пришлите пож - та стоимост...\n",
       "10336    Добрый день, Пожалуйста, забронируйте рейсы: 0...\n",
       "10910    Добрый день, Пожалуйста, забронируйте рейсы: 0...\n",
       "2562     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "2563     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "10383    Добрый день, Пожалуйста, забронируйте рейсы: 0...\n",
       "13556    NAME, добрый день, Просьба забронировать нижеу...\n",
       "12240    Добрый день, NAME Пожалуйста, забронируйте рей...\n",
       "9828     Добрый день, Пожалуйста, забронируйте рейсы: 0...\n",
       "1681     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "8602     Добрый день, Пожалуйста, забронируйте: 01.12.1...\n",
       "1678     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "1680     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "1682     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "7304     Добрый день, Пожалуйста, забронируйте следующи...\n",
       "11874    Добрый день, Пожалуйста, забронируйте рейсы: 1...\n",
       "10675    Добрый день, Пожалуйста, забронируйте рейсы: 9...\n",
       "13302    Добрый день, Пожалуйста, забронируйте рейсы: 2...\n",
       "1070     Здравствуйте, aeroclubXML. Заказ PASSPORT. NAM...\n",
       "9549     NAME, добрый день, У нас едет группа ( 30 чел....\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "79d1b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_rows = data_filtered.nlargest(50, 'pred7')\n",
    "text_column = top_10_rows['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "af960d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3291     Прошу забронировать вариант # 8, предложение #...\n",
       "8992     NAME Коллеги, прошу забронировать и прислать б...\n",
       "8764     Здравствуйте, Большое спасибо за NAME сообщени...\n",
       "10014    NAME, спасибо за информацию! Kind regards, NAM...\n",
       "11710    Заказ Академсервис PASSPORT/6159268 был измене...\n",
       "4448     Коллеги, добрый день NAME Ни как не можем найт...\n",
       "13464    NAME, NAME. Забронируйте, пожалуйста, эти пере...\n",
       "9235     NAME, подскажите, пожалуйста, по информации ни...\n",
       "7526     Добрый день! Прошу оформить авиа билеты: Номер...\n",
       "12110    Добрый день! Прошу забронировать следующие рей...\n",
       "10082    Добрый день, коллеги! Нужны билеты: Вылет 13.1...\n",
       "6667     25 ноября NAME Москва NAME NAME рейс NAME NAME...\n",
       "10825    Добрый день, Просьба бронь по маршруту – ниже ...\n",
       "5553     Заказ NAME PASSPORT/2948341 был изменен. Полар...\n",
       "11008    Заказ NAME PASSPORT/3003353 был изменен. Брать...\n",
       "62       Расчет ниже, оформляю? UCO + YCO -------------...\n",
       "12967    NAME, прошу забронировать гостиницу и рейсы на...\n",
       "5550     Заказ NAME PASSPORT/2948341 был изменен. Полар...\n",
       "7954     Коллеги, добрый день, просим прислать стоимост...\n",
       "12971    Уважаемые коллеги, Просим Вас забронировать би...\n",
       "12843    Добрый день, коллеги! Нужны билеты: Рейс из Бе...\n",
       "12318    Коллеги, NAME забронировать для NAME Сергея ( ...\n",
       "709      Добрый день, коллеги! Нужны билеты: на 22.10 р...\n",
       "12612    Заказ NAME PASSPORT/3032965 был изменен. Сэндс...\n",
       "11078    Заказ NAME PASSPORT/3015135 был изменен. Салав...\n",
       "11080    Заказ NAME PASSPORT/3015135 был изменен. Салав...\n",
       "11174    Заказ NAME PASSPORT/3015173 был изменен. ПРОМЕ...\n",
       "9658     Добрый день! Подтверждаю вариант 2: ВАРИАНТ 2 ...\n",
       "1391     NAME, а где информация по тарифам? NAME Thank ...\n",
       "10513    Добрый день, коллеги! Нужны билеты: 1. Новосиб...\n",
       "8800     Добрый день. Забронируйте, пожалуйста, билеты ...\n",
       "9759     Добрый день, уважаемые партнеры! Сообщаем Вам ...\n",
       "7156     Добрый день. Хотели уточнить возможность билет...\n",
       "11007    Добрый день, коллеги! Нужны билеты: 13.12.2019...\n",
       "797      NAME, добрый день! Ниже рейсы одной авиакомпан...\n",
       "10911    Уважаемые коллеги, Пожалуйста, забронируйте ме...\n",
       "1229     Добрый день, NAME Просим Вас взять в работу за...\n",
       "1846     Please use NAME attached credit card NAME NAME...\n",
       "1028     Добрый день, коллеги! Нужны данные билеты: 1 )...\n",
       "13858    Добрый день! Пожалуйста, оформите билет по при...\n",
       "11125    Добрый день! Прошу оформить внутренний трансфе...\n",
       "10573    Добрый день, коллеги! Нужны билеты до г. Москв...\n",
       "13257    Добрый день. Прошу подобрать билеты Москва - Ц...\n",
       "13660    NAME, не надо бронировать NAME regards, NAME N...\n",
       "12081    NAME, NAME также уточнить, есть ли возможность...\n",
       "2245     NAME, NAME Прошу забронировать билеты для NAME...\n",
       "1694     NAME, Мне нужен рейс из Франкфурта в час дня. ...\n",
       "10611    Добрый день, коллеги! Нужны билеты: Вылет с Ер...\n",
       "11338    Добрый день, NAME Подскажите, можно поменять б...\n",
       "11053    Добрый день. Прошу Вас заказать мне билеты на ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "c6d0ee99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'заявка': 73.75096079938508,\n",
       " 'запрос': 73.75096079938508,\n",
       " 'прошу': 73.75096079938508,\n",
       " 'просьба': 73.75096079938508,\n",
       " 'бронировать': 73.75096079938508,\n",
       " 'оформление': 73.75096079938508,\n",
       " 'гостиница': 73.75096079938508,\n",
       " 'командировка': 73.75096079938508,\n",
       " 'регистрация': 73.75096079938508,\n",
       " 'номер': 73.75096079938508,\n",
       " 'проживание': 73.75096079938508,\n",
       " 'рейс': 73.75096079938508,\n",
       " 'name': 73.75096079938508,\n",
       " 'email': 73.75096079938508,\n",
       " 'phone': 73.75096079938508,\n",
       " 'коллеги': 73.75096079938508,\n",
       " 'номер бронирования': 73.75096079938508,\n",
       " 'пассажиры': 73.75096079938508,\n",
       " 'вариант': 73.75096079938508,\n",
       " 'flightnum': 73.75096079938508,\n",
       " 'сообщение': 73.75096079938508,\n",
       " 'ул': 73.75096079938508,\n",
       " 'г': 73.75096079938508,\n",
       " 'аэроклуб': 73.75096079938508,\n",
       " 'компания': 73.75096079938508,\n",
       " 'рыбинский': 73.75096079938508,\n",
       " 'информация': 73.75096079938508,\n",
       " 'лаборатория': 73.75096079938508,\n",
       " 'ваш': 73.75096079938508}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "afa9f9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'заявка': 73.75096079938508,\n",
       " 'запрос': 73.75096079938508,\n",
       " 'прошу': 73.75096079938508,\n",
       " 'просьба': 73.75096079938508,\n",
       " 'бронировать': 73.75096079938508,\n",
       " 'оформление': 73.75096079938508,\n",
       " 'гостиница': 73.75096079938508,\n",
       " 'командировка': 73.75096079938508,\n",
       " 'регистрация': 73.75096079938508,\n",
       " 'номер': 73.75096079938508,\n",
       " 'проживание': 73.75096079938508,\n",
       " 'рейс': 73.75096079938508,\n",
       " 'name': 73.75096079938508,\n",
       " 'email': 73.75096079938508,\n",
       " 'phone': 73.75096079938508,\n",
       " 'коллеги': 73.75096079938508,\n",
       " 'номер бронирования': 73.75096079938508,\n",
       " 'пассажиры': 73.75096079938508,\n",
       " 'вариант': 73.75096079938508,\n",
       " 'flightnum': 73.75096079938508,\n",
       " 'ncr': 1.0,\n",
       " 'corporation': 0.9811320754716981,\n",
       " 'неолант': 0.5283018867924528,\n",
       " 'direct': 0.33962264150943394,\n",
       " 'starchenkova': 0.16981132075471697,\n",
       " 'pyy': 0.11320754716981132,\n",
       " 'выставить': 0.05660377358490566,\n",
       " 'приказ': 0.018867924528301886,\n",
       " 'drenin': 0.0}"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2680580a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
